# Scrapy基础数据存储
## 一些基础概念
1. response 是一个`from scrapy.http.response.html import HtmlResponse `类别的对象 可以执行`xpath`和`css`语法提出来的数据
2. 1中提取出来的数据 是一个`Selector`或者是一个`SelectorList`对象 如果想要获取其中的字符串 要执行`getall()`或者`get()`方法
3. `getall()`方法 获取的是`Selector`的所有文本 返回的是一个列表`list`
4. `get`方法 获取的是`Selector`中的**第一个文本** 返回的是一个`str`
5. 如果数据解析回来 要传给pipline处理 那么可以使用`yield`来返回 或者是收集全部的`item` 新建一个`items=[]` 然后使用`items.append(item)`就可以了
6. `item`必须在`items.py`中定义好模型 更规范和好处理
7. `pipline`:这个事专门来保存数据的 其中有三个方法是经常会使用的
    - `open_spider(self,spider)`: 当爬虫被打开的时候执行
    - `process_item(self,item,spider)`: 当爬虫有item传过来的时候会被调用
    - `close_spider(self,spider)`: 当爬虫关闭的时候会被调用  
    要激活`pipline` 应该在`settings.py`中 设置一下 具体设置如下：
 ```python
    ITEM_PIPELINES = {
        # 后面的数字是优先级 越小优先级越高
       'tutorial.pipelines.TutorialPipeline': 300,
    }
    
 ```
 
 ## `JsonItemExpoter`和 `JsonLinesItemExpoter`
 保存json格式数据的时候 可以使用两个类 让操作更规范
 1. `JsonItemExpoter` 每次把数据直接放入内存中 最后统一写入磁盘中 好处是 存储的数据直接满足json的规则 坏处是 很明显看内存能不能顶得住 示例如下
 
 ```python
    class TutorialPipeline:
    def __init__(self):
        print("爬虫初始化")
        self.fp = open("context.json", "w",encoding='utf-8')
        self.exporter = JsonItemExporter(self.fp, ensure_ascii=False, encoding='utf-8')
        self.exporter.start_exporting()
        pass

    def open_spider(self, spider):
        print("爬虫启动...")
        pass

    def process_item(self, item, spider):
        print("处理信息...")
        self.exporter.export_item(item)

        return item

    def close_spider(self, spider):
        self.exporter.finish_exporting()
        self.fp.close()
        print("爬虫结束...")
        pass
 
 ```
 2. `JsonLinesItemExpoter` 这个事每次调用`export_item`的时候就把这个`item`存储到硬盘中 坏处是每个字典是一行 整个文件不是一个满足`json`格式的文件 好处是每次只在内存中保留一行 不消耗内存 示例如下
 ```python
 class TutorialPipeline:
    def __init__(self):
        print("爬虫初始化")
        self.fp = open("context.json", "wb")
        self.exporter = JsonLinesItemExporter(self.fp, ensure_ascii=False, encoding='utf-8')

        pass

    def open_spider(self, spider):
        print("爬虫启动...")
        pass

    def process_item(self, item, spider):
        print("处理信息...")
        self.exporter.export_item(item)

        return item

    def close_spider(self, spider):

        self.fp.close()
        print("爬虫结束...")
        pass
 
 ```
 3. 手动进入别的链接:使用`yield scrapy.Request(url="真实的url", callback="处理前面这个url的函数")`示例如下
 ```python
    class ScrapyDemo1Spider(scrapy.Spider):
    name = 'scrapy_demo_1'
    allowed_domains = ['其实页面.com']
    start_urls = ['http://起始页面.com/']
    base_domin="demo_domin"
    def parse(self, response):
        pass
        item_divs=response.xpath("//div[@class='item_divs']/div")
        for div in item_divs:
            author=div.xpath(".//h2/text()").get().strip()
            content=div.xpath(".//div[@class='content']//text()").getall()
            content="".join(content).strip()
            item=TutorialItem(author=author,content=content)
            yield item # 这里返回item 会直接交付给pipelines处理
        next_url=response.xpath("//ul[@class='pagination']/li[last()]/a/@href").get()
        if not next_url:
            return
        else:
            # 这里yield request的话 就是返回给spider里的callback中的函数处理
            yield scrapy.Request(url=next_url,callback=self.parse)
 
 ```
 