# CrawlSpider
在一般的爬虫项目中 当我们解析完一个页面再手动获取下一个页面的url 然后重新发送一个请求   
而CrawlSpider可以只要满足条件的url,都能进去爬取规则如下：  
- 页面1
- - 页面1中的页面1.1
- - 页面1中的页面1.2  

CrawlSpider 继承自Spider 好处是不用咱手动yield Request了

## 创建crawlSpider
创建命令为：  
1. 先创建一个scrapy项目 使用`scrapy startproject [project name]`
2. 再进入`[project name]`文件夹
3. 执行 `scrapy genspider -t crawl [spider name] ["爬取的域名"]`
4. over

## LinkExtractor链接提取器
使用`LinkExtractor` 可以不用手动提取`url` 然后发送请求 这些工作都可以交给`LinkExtractor` 他会在每一页中提取满足条件的`url`然后自动爬取 至于提取出的`url`中还不要继续提取 则可以用后面的`Rule`中的`follow=True or false`来判断

```Python
    class scrapy.linkextractors.LinkExtractor(
        allow=(), # 允许的url 满足这个正则表达式中的所有rul都会被提取
        deny=(),# 禁止的url 同上 输入也是一个正则表达式
        allow_demains=(),# 允许的域名 只有在这个里面指定的域名的url才会被提取
        deny_demains=(), #同上反之
        deny_extensions=None,
        restrict_xpath=(),# 严格的xpath 与url一起过滤url
        tags=('a','area'),
        attrs=('href'),
        canonicalize=True,
        unique=True,
        process_value=None
    )
```

## Rule 规则类
定义爬虫的规则类

```python
    class scrapy.spiders.Rule(
        link_extracor,# linkextractors对象类 定义爬取规则
        callback=None,# 满足linkextractors规则的url 应该要执行callback制定的回调函数 (crawlSpider自己定了parse函数 记住不摇冲突)
        cb_kwargs=None,
        follow=None,# 满足linkextractors规则的url 还需要不需要继续提取其中满足规则的url了
        process_link=None, # 满足linkextractors规则的url会先传给这个函数 用来过滤不需要爬取的链接
        process_request=None
    )
```

## 使用范例
```python
   class CrawlSpiderSpider(CrawlSpider):
    name = 'crawl_spider'
    allowed_domains = ['wxapp-union.com']
    start_urls = ['https://www.wxapp-union.com/portal.php?mod=list&catid=2&page=1']

    rules = (
        # allow 可以爬取的页面
        # callback 可以是用什么函数去接卸allow中的页面 默认是就是parse_item
        # follow 后面的页面是否需要跟进

        # 列表页
        # https://www.wxapp-union.com/portal.php?mod=list&catid=2&page=1
        Rule(
            LinkExtractor(
                allow=r'.+mod=list&catid=2&page=\d'
            ),
            follow=True
        ),
        # 详情页
        # 'https://www.wxapp-union.com/forum-2-1.html'
        Rule(
            LinkExtractor(
                allow=r'.+article.+\.html'
            ),
            callback='parse_details',
            follow=False
        ),


    )

    def parse_details(self, response):

        # content=str(response.xpath("//div[@class='blockquote']//text()").extract_first().strip())
        content="测试字符串"
        url=str(response.url)
        # date=str(response.xpath("//span[@class='time']//text()").extract_first().strip())

        print("=="*50)
        print(f"url={url}")
        print("==" * 50)
        item = {}
        # item['url'] = url
        # item['date'] = date
        # item['content'] = content
        yield item
```

## 备注
使用`LinkExtractor`和`Rule` 这两个东西决定爬虫的走向
1. allow 设置规则的方法: 要能够限制在我们想要的url上 正则表达式不要跟其他url冲突
2. 什么时候使用`follow`: 如果在爬取页面的时候 需要将满足当前条件的url再进行跟进 就设置为`True`否则就`False`
3. 什么情况下该指定`callback`: 如果这个url对应的页面 只是为了获取更多的url 并不需要里面的数据 那么就可以不设定`callback` 反之 当我们需要对应页面中的数据的时候，那么就需要指定一个`callback`